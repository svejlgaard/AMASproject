{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hollywood-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataLoad import PulsarData\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import shap\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-algeria",
   "metadata": {},
   "source": [
    "## Classification of pulsar data using the sklearn neural network method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finished-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = PulsarData('HTRU_2').features\n",
    "targets_data = PulsarData('HTRU_2').targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-answer",
   "metadata": {},
   "source": [
    "Shuffle and split the data into training and test groups with 3:1 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hundred-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_data, test_features_data, train_targets_data, test_targets_data  =  train_test_split( features_data, \n",
    "                                                        targets_data, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-lending",
   "metadata": {},
   "source": [
    "Bayesian optimisation and cross validation of hyperparameters using Simone's Troels example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outdoor-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklNN_CrossValidation(hidden_layer_sizes, learning_rate_init, data, targets):\n",
    "    \"\"\"Cross validation.\n",
    "       Fits a NN with the given paramaters to the target \n",
    "       given data, calculated a CV accuracy score and returns the mean.\n",
    "       The goal is to find combinations\n",
    "       that maximize the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    estimator = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=learning_rate_init, random_state=0)\n",
    "    \n",
    "    cval = cross_val_score(estimator, data, targets, scoring='accuracy', cv=5)\n",
    "    \n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thermal-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_sklNN(data, targets, pars, n_iter=5):\n",
    "    \"\"\"Apply Bayesian Optimization to NN parameters.\"\"\"\n",
    "    \n",
    "    def crossval_wrapper(hidden_layer_sizes, learning_rate_init):\n",
    "        \"\"\"Wrapper of NNe cross validation. \n",
    "           hidden_layer_sizes\n",
    "           is cast to integer before we pass them along.\n",
    "        \"\"\"\n",
    "        return sklNN_CrossValidation(hidden_layer_sizes=int(hidden_layer_sizes), \n",
    "                                            learning_rate_init=learning_rate_init, \n",
    "                                            data=data, \n",
    "                                            targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(f=crossval_wrapper, \n",
    "                                     pbounds=pars, \n",
    "                                     random_state=42, \n",
    "                                     verbose=2)\n",
    "    optimizer.maximize(init_points=4, n_iter=n_iter)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "established-luxembourg",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|   iter    |  target   | hidden... | learni... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9766  \u001b[0m | \u001b[0m 187.9   \u001b[0m | \u001b[0m 0.9507  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9762  \u001b[0m | \u001b[0m 366.3   \u001b[0m | \u001b[0m 0.5987  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9794  \u001b[0m | \u001b[95m 78.85   \u001b[0m | \u001b[95m 0.1561  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9758  \u001b[0m | \u001b[0m 29.98   \u001b[0m | \u001b[0m 0.8662  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9791  \u001b[0m | \u001b[0m 1.026   \u001b[0m | \u001b[0m 0.9539  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9779  \u001b[0m | \u001b[0m 83.83   \u001b[0m | \u001b[0m 0.6154  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9779  \u001b[0m | \u001b[0m 1.004   \u001b[0m | \u001b[0m 0.2526  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9791  \u001b[0m | \u001b[0m 1.075   \u001b[0m | \u001b[0m 0.7396  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.977   \u001b[0m | \u001b[0m 1.015   \u001b[0m | \u001b[0m 0.4597  \u001b[0m |\n",
      "=================================================\n",
      "{'target': 0.9794381492366655, 'params': {'hidden_layer_sizes': 78.85330158077582, 'learning_rate_init': 0.15607892088416903}}\n"
     ]
    }
   ],
   "source": [
    "parameters_BayesianOptimization = {\"hidden_layer_sizes\": (1, 500), \n",
    "                                   \"learning_rate_init\": (0.0001, 1)\n",
    "                                  }\n",
    "\n",
    "BayesianOptimization = optimize_sklNN(train_features_data, \n",
    "                                             train_targets_data, \n",
    "                                             parameters_BayesianOptimization, \n",
    "                                             n_iter=5)\n",
    "print(BayesianOptimization.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-wedding",
   "metadata": {},
   "source": [
    "Cross-validation on result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "internal-jersey",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8698 accuracy with a standard deviation of 0.0186\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=int(BayesianOptimization.max['params']['hidden_layer_sizes']), \n",
    "                                 learning_rate_init=BayesianOptimization.max['params']['learning_rate_init'],\n",
    "                                 random_state=0)\n",
    "scores = cross_val_score(clf, features_data, targets_data, cv=5, scoring='f1') \n",
    "print(f\"{scores.mean():.4f} accuracy with a standard deviation of {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-richardson",
   "metadata": {},
   "source": [
    "Comparing with baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convenient-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.0386 improvement with a standard deviation of 0.0186\n"
     ]
    }
   ],
   "source": [
    "print(f\"{scores.mean()-PulsarData('HTRU_2').baseline:.4f} improvement with a standard deviation of {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-lindsay",
   "metadata": {},
   "source": [
    "Fit a gradient boosting classifier with hyperparameters optimised by Gaussian Process Optimisation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "derived-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fit = clf.fit(train_features_data, train_targets_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-franklin",
   "metadata": {},
   "source": [
    "Compare the classified data to the test set - returns percentage match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "monthly-arrow",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9781005586592179"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "clf_fit.score(test_features_data, test_targets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "central-support",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\nkeras is no longer supported, please use tf.keras instead.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'> is not currently a supported model type!",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fd9c847f9478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_phase_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyTorchDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# determine the model inputs and outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The model output to be explained must be a single tensor!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/shap/explainers/tf_utils.py\u001b[0m in \u001b[0;36m_get_model_inputs\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is not currently a supported model type!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'> is not currently a supported model type!"
     ]
    }
   ],
   "source": [
    "explainer = shap.DeepExplainer(clf_fit, train_features_data).shap_values(train_features_data)\n",
    "shap.summary_plot(shap_values, train_features_data, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-tongue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d15befdc11df375b4634f1adc5c62eddc2db8e0dc0cec2fa4dd6447847bd678d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}