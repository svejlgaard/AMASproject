{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "comprehensive-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataLoad import PulsarData\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-fraction",
   "metadata": {},
   "source": [
    "## Classification of pulsar data using the sklearn neural network method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "great-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = PulsarData('HTRU_2').features\n",
    "targets_data = PulsarData('HTRU_2').targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-exchange",
   "metadata": {},
   "source": [
    "Shuffle and split the data into training and test groups with 3:1 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "saved-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_data, test_features_data, train_targets_data, test_targets_data  =  train_test_split( features_data, \n",
    "                                                        targets_data, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-chinese",
   "metadata": {},
   "source": [
    "Bayesian optimisation and cross validation of hyperparameters using Simone's Troels example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "extended-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklNN_CrossValidation(hidden_layer_sizes, learning_rate_init, data, targets):\n",
    "    \"\"\"Cross validation.\n",
    "       Fits a NN with the given paramaters to the target \n",
    "       given data, calculated a CV accuracy score and returns the mean.\n",
    "       The goal is to find combinations\n",
    "       that maximize the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    estimator = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=learning_rate_init, random_state=0)\n",
    "    \n",
    "    cval = cross_val_score(estimator, data, targets, scoring='accuracy', cv=5)\n",
    "    \n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "differential-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_sklNN(data, targets, pars, n_iter=5):\n",
    "    \"\"\"Apply Bayesian Optimization to NN parameters.\"\"\"\n",
    "    \n",
    "    def crossval_wrapper(hidden_layer_sizes, learning_rate_init):\n",
    "        \"\"\"Wrapper of NNe cross validation. \n",
    "           hidden_layer_sizes\n",
    "           is cast to integer before we pass them along.\n",
    "        \"\"\"\n",
    "        return sklNN_CrossValidation(hidden_layer_sizes=int(hidden_layer_sizes), \n",
    "                                            learning_rate_init=learning_rate_init, \n",
    "                                            data=data, \n",
    "                                            targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(f=crossval_wrapper, \n",
    "                                     pbounds=pars, \n",
    "                                     random_state=42, \n",
    "                                     verbose=2)\n",
    "    optimizer.maximize(init_points=4, n_iter=n_iter)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "after-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | hidden... | learni... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 187.9   \u001b[0m | \u001b[0m 0.9507  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.9569  \u001b[0m | \u001b[95m 366.3   \u001b[0m | \u001b[95m 0.5987  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9727  \u001b[0m | \u001b[95m 78.85   \u001b[0m | \u001b[95m 0.1561  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 29.98   \u001b[0m | \u001b[0m 0.8662  \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.9761  \u001b[0m | \u001b[95m 500.0   \u001b[0m | \u001b[95m 0.0001  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9755  \u001b[0m | \u001b[0m 496.5   \u001b[0m | \u001b[0m 0.1832  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9755  \u001b[0m | \u001b[0m 428.1   \u001b[0m | \u001b[0m 0.01407 \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 114.0   \u001b[0m | \u001b[0m 0.9893  \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.9765  \u001b[0m | \u001b[95m 274.3   \u001b[0m | \u001b[95m 0.008437\u001b[0m |\n",
      "=================================================\n",
      "{'target': 0.9765330380459971, 'params': {'hidden_layer_sizes': 274.3146737438626, 'learning_rate_init': 0.00843679678392597}}\n"
     ]
    }
   ],
   "source": [
    "parameters_BayesianOptimization = {\"hidden_layer_sizes\": (1, 500), \n",
    "                                   \"learning_rate_init\": (0.0001, 1)\n",
    "                                  }\n",
    "\n",
    "BayesianOptimization = optimize_sklNN(train_features_data, \n",
    "                                             train_targets_data, \n",
    "                                             parameters_BayesianOptimization, \n",
    "                                             n_iter=5)\n",
    "print(BayesianOptimization.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-distinction",
   "metadata": {},
   "source": [
    "Cross-validation on result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "subject-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9766 accuracy with a standard deviation of 0.0023\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=int(BayesianOptimization.max['params']['hidden_layer_sizes']), \n",
    "                                 learning_rate_init=BayesianOptimization.max['params']['learning_rate_init'],\n",
    "                                 random_state=0)\n",
    "scores = cross_val_score(clf, features_data, targets_data, cv=5) \n",
    "print(f\"{scores.mean():.4f} accuracy with a standard deviation of {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-moses",
   "metadata": {},
   "source": [
    "Comparing with baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "freelance-conflict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0682 improvement with a standard deviation of 0.0023\n"
     ]
    }
   ],
   "source": [
    "print(f\"{scores.mean()-PulsarData('HTRU_2').baseline:.4f} improvement with a standard deviation of {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-nightlife",
   "metadata": {},
   "source": [
    "Fit a gradient boosting classifier with hyperparameters optimised by Gaussian Process Optimisation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "headed-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fit = clf.fit(train_features_data, train_targets_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-reservation",
   "metadata": {},
   "source": [
    "Compare the classified data to the test set - returns percentage match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "latter-policy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9776536312849162"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_fit.score(test_features_data, test_targets_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-lithuania",
   "metadata": {},
   "source": [
    "SHAP-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "confident-purchase",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Model type not yet supported by TreeExplainer: <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b4113034e487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/amas/lib/python3.8/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, **deprecated_options)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_perturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/amas/lib/python3.8/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model type not yet supported by TreeExplainer: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;31m# build a dense numpy version of all the tree objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Model type not yet supported by TreeExplainer: <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>"
     ]
    }
   ],
   "source": [
    "shap_values = shap.TreeExplainer(clf_fit).shap_values(train_features_data)\n",
    "shap.summary_plot(shap_values, train_features_data, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-stomach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amas",
   "language": "python",
   "name": "amas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
