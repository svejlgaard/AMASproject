{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hollywood-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataLoad import PulsarData\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import shap\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-algeria",
   "metadata": {},
   "source": [
    "## Classification of pulsar data using the sklearn neural network method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finished-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = PulsarData('HTRU_2').features\n",
    "targets_data = PulsarData('HTRU_2').targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-answer",
   "metadata": {},
   "source": [
    "Shuffle and split the data into training and test groups with 3:1 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hundred-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_data, test_features_data, train_targets_data, test_targets_data  =  train_test_split( features_data, \n",
    "                                                        targets_data, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-lending",
   "metadata": {},
   "source": [
    "Bayesian optimisation and cross validation of hyperparameters using Simone's Troels example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outdoor-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklNN_CrossValidation(hidden_layer_sizes, learning_rate_init, data, targets):\n",
    "    \"\"\"Cross validation.\n",
    "       Fits a NN with the given paramaters to the target \n",
    "       given data, calculated a CV accuracy score and returns the mean.\n",
    "       The goal is to find combinations\n",
    "       that maximize the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    estimator = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=learning_rate_init, random_state=0)\n",
    "    \n",
    "    cval = cross_val_score(estimator, data, targets, scoring='accuracy', cv=5)\n",
    "    \n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thermal-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_sklNN(data, targets, pars, n_iter=5):\n",
    "    \"\"\"Apply Bayesian Optimization to NN parameters.\"\"\"\n",
    "    \n",
    "    def crossval_wrapper(hidden_layer_sizes, learning_rate_init):\n",
    "        \"\"\"Wrapper of NNe cross validation. \n",
    "           hidden_layer_sizes\n",
    "           is cast to integer before we pass them along.\n",
    "        \"\"\"\n",
    "        return sklNN_CrossValidation(hidden_layer_sizes=int(hidden_layer_sizes), \n",
    "                                            learning_rate_init=learning_rate_init, \n",
    "                                            data=data, \n",
    "                                            targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(f=crossval_wrapper, \n",
    "                                     pbounds=pars, \n",
    "                                     random_state=42, \n",
    "                                     verbose=2)\n",
    "    optimizer.maximize(init_points=4, n_iter=n_iter)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "established-luxembourg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | hidden... | learni... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9766  \u001b[0m | \u001b[0m 187.9   \u001b[0m | \u001b[0m 0.9507  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9762  \u001b[0m | \u001b[0m 366.3   \u001b[0m | \u001b[0m 0.5987  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9794  \u001b[0m | \u001b[95m 78.85   \u001b[0m | \u001b[95m 0.1561  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9758  \u001b[0m | \u001b[0m 29.98   \u001b[0m | \u001b[0m 0.8662  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9791  \u001b[0m | \u001b[0m 1.026   \u001b[0m | \u001b[0m 0.9539  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9779  \u001b[0m | \u001b[0m 83.83   \u001b[0m | \u001b[0m 0.6154  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9779  \u001b[0m | \u001b[0m 1.004   \u001b[0m | \u001b[0m 0.2526  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9791  \u001b[0m | \u001b[0m 1.075   \u001b[0m | \u001b[0m 0.7396  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.977   \u001b[0m | \u001b[0m 1.015   \u001b[0m | \u001b[0m 0.4597  \u001b[0m |\n",
      "=================================================\n",
      "{'target': 0.9794381492366655, 'params': {'hidden_layer_sizes': 78.85330158077582, 'learning_rate_init': 0.15607892088416903}}\n"
     ]
    }
   ],
   "source": [
    "parameters_BayesianOptimization = {\"hidden_layer_sizes\": (1, 500), \n",
    "                                   \"learning_rate_init\": (0.0001, 1)\n",
    "                                  }\n",
    "\n",
    "BayesianOptimization = optimize_sklNN(train_features_data, \n",
    "                                             train_targets_data, \n",
    "                                             parameters_BayesianOptimization, \n",
    "                                             n_iter=5)\n",
    "print(BayesianOptimization.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-wedding",
   "metadata": {},
   "source": [
    "Cross-validation on result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "internal-jersey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8698 accuracy with a standard deviation of 0.0186\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=int(BayesianOptimization.max['params']['hidden_layer_sizes']), \n",
    "                                 learning_rate_init=BayesianOptimization.max['params']['learning_rate_init'],\n",
    "                                 random_state=0)\n",
    "scores = cross_val_score(clf, features_data, targets_data, cv=5, scoring='f1') \n",
    "print(f\"{scores.mean():.4f} accuracy with a standard deviation of {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-richardson",
   "metadata": {},
   "source": [
    "Comparing with baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convenient-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0386 improvement with a standard deviation of 0.0186\n"
     ]
    }
   ],
   "source": [
    "print(f\"{scores.mean()-PulsarData('HTRU_2').baseline:.4f} improvement with a standard deviation of {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-lindsay",
   "metadata": {},
   "source": [
    "Fit a gradient boosting classifier with hyperparameters optimised by Gaussian Process Optimisation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "derived-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fit = clf.fit(train_features_data, train_targets_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-franklin",
   "metadata": {},
   "source": [
    "Compare the classified data to the test set - returns percentage match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "monthly-arrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9781005586592179"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_fit.score(test_features_data, test_targets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "regulated-commitment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851b6ebcb74c4064875780c2bbfaf20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "You must pass an Explanation object, Cohorts object, or dictionary to bar plot!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7068bfef0000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKernelExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/shap/plots/_bar.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(shap_values, max_display, order, clustering, clustering_cutoff, merge_cohorts, show_data, show)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mcohorts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcohorts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You must pass an Explanation object, Cohorts object, or dictionary to bar plot!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# unpack our list of Explanation objects we need to plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: You must pass an Explanation object, Cohorts object, or dictionary to bar plot!"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=int(BayesianOptimization.max['params']['hidden_layer_sizes']), \n",
    "                                 learning_rate_init=BayesianOptimization.max['params']['learning_rate_init'],\n",
    "                                 random_state=0)\n",
    "model.fit(test_features_data, test_targets_data)\n",
    "explainer = shap.KernelExplainer(model.predict_proba, shap.sample(test_features_data,100), link=\"logit\")\n",
    "shap_values = explainer.shap_values(test_features_data, nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "central-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "new_shape = np.vstack(np.array(shap_values))\n",
    "shap.summary_plot(new_shape, test_features_data, plot_type=\"bar\", show='False')\n",
    "plt.savefig('plots/NN_SHAP.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-tongue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
