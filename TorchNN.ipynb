{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataLoad import PulsarData\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pretty plotting\n",
    "plt.style.use('seaborn-paper')\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "source": [
    "Creating the neural network:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralN(nn.Module):\n",
    "    def __init__(self,inputsize,hiddensize):\n",
    "        super(NeuralN, self).__init__()\n",
    "        self.inputsize=inputsize\n",
    "        self.hiddensize=hiddensize\n",
    "        # an affine operation: y = Wx + b, this is basically a weight tensor!\n",
    "        self.fcinput = nn.Linear(in_features=self.inputsize, out_features=self.hiddensize)\n",
    "        self.fcoutput = nn.Linear(in_features=self.hiddensize, out_features=2)\n",
    "    \n",
    "    def forward(self,x: torch.Tensor):\n",
    "        x = self.fcinput(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fcoutput(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "Loading the data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features = PulsarData('HTRU_2').features\n",
    "raw_targets = PulsarData('HTRU_2').targets"
   ]
  },
  {
   "source": [
    "Defining the epochs for the neural network to train:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "source": [
    "Splitting data into test and train data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_data, test_features_data, train_targets_data, test_targets_data  =  train_test_split( raw_features, \n",
    "                                                        raw_targets, test_size=0.25, random_state=42)"
   ]
  },
  {
   "source": [
    "Writing a cross validation function that is compatible with torch: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_CrossValidation(hiddensize, learning_rate, data, targets):\n",
    "   cv = 5\n",
    "   net = NeuralN(data.shape[1], hiddensize)\n",
    "   dlist = np.array_split(data.to_numpy(), cv)\n",
    "   tlist = np.array_split(targets.to_numpy(), cv)\n",
    "   cval = list()\n",
    "   for d, dat in tqdm(enumerate(dlist)):\n",
    "      cross_dlist =  dlist[:d] + dlist[d+1 :]\n",
    "      cross_tlist =  tlist[:d] + tlist[d+1 :]\n",
    "      cross_dat = torch.from_numpy(np.concatenate(cross_dlist)).float()\n",
    "      cross_tar = torch.from_numpy(np.concatenate(cross_tlist)).long()\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "      net.train()\n",
    "      for e in range(epochs):\n",
    "         epoch_losses = list()\n",
    "         for n in range(cross_dat.shape[0]):\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad() \n",
    "            prediction = net(cross_dat[n]).unsqueeze(0)\n",
    "            target = cross_tar[n].unsqueeze(0)\n",
    "            # Calculating the loss function\n",
    "            loss = criterion(prediction,target)\n",
    "            epoch_losses.append(float(loss))\n",
    "            # Calculating the gradient\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "      net.eval()\n",
    "      cross_pred = torch.argmax(net(torch.from_numpy(dat).float()),dim=1)\n",
    "      acc_cross = torch.mean((cross_pred == torch.from_numpy(tlist[d]).long()).float())\n",
    "      cval.append(acc_cross)\n",
    "\n",
    "   return np.mean(np.array(cval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_NN(data, targets, pars, n_iter=5):\n",
    "    \"\"\"Apply Bayesian Optimization to Neural Network parameters.\"\"\"\n",
    "    \n",
    "    def crossval_wrapper(hiddensize, learning_rate):\n",
    "        \"\"\"Wrapper of Neural Network cross validation. \n",
    "           Notice how we ensure params are casted to integer before we pass them along.\n",
    "        \"\"\"\n",
    "        return NN_CrossValidation(hiddensize=int(hiddensize), \n",
    "                                            learning_rate=learning_rate, \n",
    "                                            data=data, \n",
    "                                            targets=targets)\n",
    "\n",
    "    boptimizer = BayesianOptimization(f=crossval_wrapper, \n",
    "                                     pbounds=pars, \n",
    "                                     random_state=42, \n",
    "                                     verbose=2)\n",
    "    boptimizer.maximize(init_points=4, n_iter=n_iter)\n",
    "\n",
    "    return boptimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]|   iter    |  target   | hidden... | learni... |\n",
      "-------------------------------------------------\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.37s/it]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.45s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 19.35   \u001b[0m | \u001b[0m 0.4754  \u001b[0m |\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.50s/it]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.65s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 36.87   \u001b[0m | \u001b[0m 0.2993  \u001b[0m |\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.72s/it]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.95s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 8.645   \u001b[0m | \u001b[0m 0.07801 \u001b[0m |\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.86s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.04s/it]\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 3.846   \u001b[0m | \u001b[0m 0.4331  \u001b[0m |\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.99s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.11s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 0.08577 \u001b[0m |\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.26s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.15s/it]\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9569  \u001b[0m | \u001b[95m 28.06   \u001b[0m | \u001b[95m 0.04473 \u001b[0m |\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.28s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.23s/it]\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 28.99   \u001b[0m | \u001b[0m 0.4956  \u001b[0m |\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.27s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.35s/it]\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9081  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.29s/it]\n",
      "100%|██████████| 2/2 [00:11<00:00,  5.51s/it]| \u001b[95m 9       \u001b[0m | \u001b[95m 0.9684  \u001b[0m | \u001b[95m 44.03   \u001b[0m | \u001b[95m 1e-05   \u001b[0m |\n",
      "=================================================\n",
      "{'target': 0.9684126973152161, 'params': {'hiddensize': 44.02696538592287, 'learning_rate': 1e-05}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters_BayesianOptimization = {\"hiddensize\": (1, 50), \n",
    "                                   \"learning_rate\": (0.00001, 0.05),\n",
    "                                  }\n",
    "\n",
    "BayesianOptimization = optimize_NN(train_features_data, \n",
    "                                             train_targets_data, \n",
    "                                             parameters_BayesianOptimization, \n",
    "                                             n_iter=5)\n",
    "print(BayesianOptimization.max)"
   ]
  },
  {
   "source": [
    "Creating a neural network with the optimal hiddensize:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralN(raw_features.shape[1], int(BayesianOptimization.max['params']['hiddensize']))"
   ]
  },
  {
   "source": [
    "Converting data into torch tensors:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_data, test_features_data = torch.from_numpy(train_features_data.to_numpy()).float(), torch.from_numpy(test_features_data.to_numpy()).float()\n",
    "train_targets_data, test_targets_data = torch.from_numpy(train_targets_data.to_numpy()).long(), torch.from_numpy(test_targets_data.to_numpy()).long()"
   ]
  },
  {
   "source": [
    "Setting the optimal learning rate and training the network:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NeuralN(\n",
       "  (fcinput): Linear(in_features=8, out_features=44, bias=True)\n",
       "  (fcoutput): Linear(in_features=44, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=BayesianOptimization.max['params']['learning_rate'])\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.7955809585704927\n",
      "1 0.11341940348229061\n",
      "2 0.09552984293327933\n",
      "3 0.09223155197848301\n",
      "4 0.09053195488000564\n",
      "5 0.08938073620988715\n",
      "6 0.08853963015316019\n",
      "7 0.08788191352316818\n",
      "8 0.08734193770000165\n",
      "9 0.08688318341871432\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NeuralN(\n",
       "  (fcinput): Linear(in_features=8, out_features=44, bias=True)\n",
       "  (fcoutput): Linear(in_features=44, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    epoch_losses = list()\n",
    "    for n in range(train_features_data.shape[0]):\n",
    "        net.zero_grad()\n",
    "        optimizer.zero_grad() \n",
    "        prediction = net(train_features_data[n]).unsqueeze(0)\n",
    "        target = train_targets_data[n].unsqueeze(0)\n",
    "        # Calculating the loss function\n",
    "        loss = criterion(prediction,target)\n",
    "        epoch_losses.append(float(loss))\n",
    "        # Calculating the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(e, np.mean(epoch_losses))\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "source": [
    "Final result for train data and test data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.9747) tensor(0.9736)\n"
     ]
    }
   ],
   "source": [
    "train_prediction = torch.argmax(net(train_features_data),dim=1)\n",
    "acc_train = torch.mean((train_prediction == train_targets_data).float())\n",
    "test_prediction = torch.argmax(net(test_features_data),dim=1)\n",
    "acc_test = torch.mean((test_prediction == test_targets_data).float())\n",
    "\n",
    "print(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}