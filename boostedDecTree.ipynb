{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "secondary-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataLoad import PulsarData\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-tournament",
   "metadata": {},
   "source": [
    "Read in the pulsar data from the dataLoad.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "detected-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = PulsarData('HTRU_2').features\n",
    "targets_data = PulsarData('HTRU_2').targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-decrease",
   "metadata": {},
   "source": [
    "Shuffle and split the data into training and test groups with 3:1 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sticky-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_data, test_features_data, train_targets_data, test_targets_data  =  train_test_split( features_data, \n",
    "                                                        targets_data, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "veterinary-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBC_CrossValidation(n_estimators, learning_rate, max_depth, data, targets):\n",
    "    \"\"\"Decision Tree cross validation.\n",
    "       Fits a Decision Tree with the given paramaters to the target \n",
    "       given data, calculated a CV accuracy score and returns the mean.\n",
    "       The goal is to find combinations of max_depth, min_samples_leaf \n",
    "       that maximize the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    estimator = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \n",
    "                                 max_depth=max_depth, random_state=0)\n",
    "    \n",
    "    cval = cross_val_score(estimator, data, targets, scoring='accuracy', cv=5)\n",
    "    \n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "spread-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_GBC(data, targets, pars, n_iter=5):\n",
    "    \"\"\"Apply Bayesian Optimization to Decision Tree parameters.\"\"\"\n",
    "    \n",
    "    def crossval_wrapper(n_estimators, learning_rate, max_depth):\n",
    "        \"\"\"Wrapper of Decision Tree cross validation. \n",
    "           Notice how we ensure max_depth, min_samples_leaf \n",
    "           are casted to integer before we pass them along.\n",
    "        \"\"\"\n",
    "        return GBC_CrossValidation(n_estimators=int(n_estimators), \n",
    "                                            learning_rate=learning_rate, \n",
    "                                            max_depth=int(max_depth),\n",
    "                                            data=data, \n",
    "                                            targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(f=crossval_wrapper, \n",
    "                                     pbounds=pars, \n",
    "                                     random_state=42, \n",
    "                                     verbose=2)\n",
    "    optimizer.maximize(init_points=4, n_iter=n_iter)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "rough-vanilla",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9754  \u001b[0m | \u001b[0m 0.3746  \u001b[0m | \u001b[0m 4.803   \u001b[0m | \u001b[0m 379.4   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.98    \u001b[0m | \u001b[95m 0.5987  \u001b[0m | \u001b[95m 1.624   \u001b[0m | \u001b[95m 120.2   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9788  \u001b[0m | \u001b[0m 0.05818 \u001b[0m | \u001b[0m 4.465   \u001b[0m | \u001b[0m 320.5   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9794  \u001b[0m | \u001b[0m 0.7081  \u001b[0m | \u001b[0m 1.082   \u001b[0m | \u001b[0m 486.5   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.978   \u001b[0m | \u001b[0m 0.045   \u001b[0m | \u001b[0m 1.427   \u001b[0m | \u001b[0m 50.11   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9785  \u001b[0m | \u001b[0m 0.2694  \u001b[0m | \u001b[0m 1.39    \u001b[0m | \u001b[0m 50.08   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9709  \u001b[0m | \u001b[0m 0.697   \u001b[0m | \u001b[0m 3.884   \u001b[0m | \u001b[0m 50.08   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9799  \u001b[0m | \u001b[0m 0.3956  \u001b[0m | \u001b[0m 1.008   \u001b[0m | \u001b[0m 247.2   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9799  \u001b[0m | \u001b[0m 0.6653  \u001b[0m | \u001b[0m 1.004   \u001b[0m | \u001b[0m 343.3   \u001b[0m |\n",
      "=============================================================\n",
      "{'target': 0.979959536754115, 'params': {'learning_rate': 0.5986986183486169, 'max_depth': 1.624074561769746, 'n_estimators': 120.1975341512912}}\n"
     ]
    }
   ],
   "source": [
    "parameters_BayesianOptimization = {\"learning_rate\": (0.0001, 1), \n",
    "                                   \"max_depth\": (1, 5),\n",
    "                                   \"n_estimators\": (50,500)\n",
    "                                  }\n",
    "\n",
    "BayesianOptimization = optimize_GBC(train_features_data, \n",
    "                                             train_targets_data, \n",
    "                                             parameters_BayesianOptimization, \n",
    "                                             n_iter=5)\n",
    "print(BayesianOptimization.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-potential",
   "metadata": {},
   "source": [
    "Fit a gradient boosting classifier with 100 decision stumps as weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "historic-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, \n",
    "                                 max_depth=1, random_state=0).fit(train_features_data, train_targets_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-hours",
   "metadata": {},
   "source": [
    "Compare the classified data to the test set - returns percentage match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aboriginal-morocco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787709497206704"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_features_data, test_targets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-lloyd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amas",
   "language": "python",
   "name": "amas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
